\documentclass[norsk,a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx} %for å inkludere grafikk
\usepackage{verbatim} %for å inkludere filer med tegn LaTeX ikke liker
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{float}
\usepackage{color}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{tikz}

\lstset{language=c++}
\lstset{basicstyle=\small}
\lstset{backgroundcolor=\color{white}}
\lstset{frame=single}
\lstset{stringstyle=\ttfamily}
\lstset{keywordstyle=\color{red}\bfseries}
\lstset{commentstyle=\itshape\color{blue}}
\lstset{showspaces=false}
\lstset{showstringspaces=false}
\lstset{showtabs=false}
\lstset{breaklines}
\lstset{postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{red}\hookrightarrow\space}}}
\usepackage{titlesec}

\setcounter{secnumdepth}{4}
\usetikzlibrary{through,calc,er,positioning}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}


\title{FYS4411 - Computational Physics II\\\vspace{2mm} \Large{Project 2}}
\author{\large Dorthea Gjestvang\\ Even Marius Nordhagen}
\date\today
\begin{document}

\maketitle

\begin{itemize}
\item Github repository containing programs and results: \\\url{https://github.com/evenmn/FYS4411/tree/master/Project%202}
\end{itemize}

\begin{abstract}
Abstract
\par 

\end{abstract}

\newpage

\tableofcontents

\newpage

\section{Introduction} \label{sec:Introduction}


\section{Theory} \label{sec:Theory}

\subsection{Presentation of potential and trial wavefunction} 

We study a system of $N$ bosons trapped in a harmonic oscillator with the Hamiltonian given by 
\begin{equation}
\hat{H}=\sum_i^N\bigg(-\frac{\hbar^2}{2m}\nabla_i^2+V_{ext}(\vec{r}_i)\bigg)+\sum_{i<j}^NV_{int}(\vec{r}_i,\vec{r}_j)
\label{eq:Hamilton}
\end{equation}
with $V_{ext}$ as the external potential, which is the harmonic oscillator potential defined in equation \ref{eq:V_ext}, and $V_{int}$ as the interaction term, defined in equation \ref{eq:V_int}, which ensures the particles to be separated by a distance $a$. $\hbar$ is the reduced Plancks constant and $m$ is the mass of the particles in question. In this project we study the gas of $^{87}$Rb alkali atoms, which have a scattering length $a_{\text{Rb}}=0.0043$ given in units of a typical trap size $a_{ho}$. We will consider a harmonic oscillator which can either be spherical, where all dimensions have the same scales and the harmonic oscillator frequency is $\omega_{ho}$, or elliptical, where the z dimension has a different frequency $\omega_z$.

\begin{equation}
\label{eq:V_ext}
V_{ext}(\vec{r})=
\begin{cases} 
   \frac{1}{2}m\omega_{ho}^2\vec{r}^2 & \text{(Spherical)} \\
   \frac{1}{2}m[\omega_{ho}^2(x^2 + y^2) + \omega_z^2z^2] & \text{(Elliptical)}.
\end{cases}
\end{equation}

\begin{equation}
\label{eq:V_int}
V_{int}(\vec{r}_i, \vec{r}_j)=
\begin{cases} 
0 & \text{if}\quad |\vec{r}_i-\vec{r}_j| \geq a \\
\infty & \text{if}\quad |\vec{r}_i-\vec{r}_j| < a
\end{cases}
\end{equation}
The trial wavefunction is on the form 
\begin{equation}
\Psi_T(\vec{r}_1, \vec{r}_2, ..., \vec{r}_N, \alpha, \beta)=\prod_i^Ng(\alpha, \beta, \vec{r}_i)\prod_{i<j}f(a,r_{ij})
\label{eq:WF}
\end{equation}
where $r_{ij}=|\vec{r}_i-\vec{r}_j|$ and $g$ is assumed to be a gaussian function,
\begin{equation}
g(\alpha, \beta, \vec{r}_i)=\exp[-\alpha(x_i^2+y_i^2+\beta z_i^2)],
\end{equation}
which is practical since
\begin{equation}
\prod_i^Ng(\alpha, \beta, \vec{r}_i)=\exp\Big[-\alpha\sum_{i=1}^N(x_i^2+y_i^2+\beta z_i^2)\Big].
\end{equation}
$\alpha$ is a variational parameter that we later use to find the energy minimum, and $\beta$ is a constant. The trial wave function used is the exact ground state wave function for a harmonic oscillator with no interaction, so by choosing the correct $\alpha$, we will find the exact ground state energy. The $\alpha$ for the ground state wave function in this case is known to be $0.5$. It is sufficient to study the ground state wave function only, since all particles occupy this state in a Bose-Einstein condensation.

The $f$ presented above is the correlation wave function, which is 
\begin{equation}
\label{eq:WF_interaction_part}
f(a,r_{ij})=
\begin{cases} 
   0 & r_{ij} \leq a \\
   \left(1-\frac{a}{r_{ij}}\right) & r_{ij} > a.
\end{cases}
\end{equation}
where $a$ is a parameter describing the minimum distance allowed between particles in the trap. This is also denoted as the Jastrow factor, and it sets the wave function to zero when the distance between two particles is smaller than $a$, such that we get a hard-sphere repulsive potential. 

\subsection{Energy calculation}

We want to find the ground state energy of our system for a given wave function $\Psi_T$. The energy is given as the expectation value of the Hamiltonian, given by equation \ref{eq:general_energy}. 

\begin{equation}
E=\langle H\rangle = \frac{\int\Psi^*(\vec{r})H\Psi(\vec{r})d\vec{r}}{\int\Psi^*(\vec{r})\Psi(\vec{r})d\vec{r}}
\label{eq:general_energy}
\end{equation}
By defining a quantity called the local energy $E_L$ given by equation \ref{eq:Local_energy}

\begin{equation}
E_L(\vec{r})=\frac{1}{\Psi_T(\vec{r})}\hat{H}\Psi_T(\vec{r}).
\label{eq:Local_energy}
\end{equation}
the energy can be written as

\begin{equation}
	E = \int | \Psi_T^2| E_L d\vec{r}
\end{equation}

This equation can be solved by calculating the local energy $E_{L,i}$ for each iteration in the Monte Carlo loop, summing them, and then divide by the number of Monte Carlo cycles:

\begin{equation}
	E = \frac{\sum_i^M E_{L,i}}{M}
\end{equation}

 We want to calculate the energy E as a function of $\alpha$ for the trial wave function given in equation \ref{eq:WF} by using Variational Monte Carlo (VMC) described in section \ref{VMC}. 

When the repulsive interaction is ignored ($a=0$), it can be shown that the local energy for a system of $N$ particles and $dim$ spatial dimensions is given by
\begin{equation}
E_L=dim\cdot N\cdot \alpha + \Big(\frac{1}{2}-2\alpha^2\Big)\sum_i\vec{r}_i^2,
\end{equation}
which is proven in appendix A. This is only true for the a spherical harmonic oscillator trap, for the elliptical trap we need to add $\beta$ in front of the z-component, which is also given in appendix A. You may also notice that this equation is scaled, more about that in section \ref{sec:scaling}.

For the a spherical harmonic oscillator where particle interactions are ignored, the analytical expression for the energy is well-known and reads $E = \hbar\omega(n + dim/2)$ where $n$ is the energy level and $dim$ is number of spatial dimensions. In this project we will study the ground state only, such that $n=0$, and for $N$ particles and $dim$ spatial dimensions we therefore obtain the expression for the ground state energy shown in equation \ref{eq:Energy_exact}.

\begin{equation}
E = \frac{1}{2}N\cdot dim\cdot\hbar\omega_{ho}.
\label{eq:Energy_exact}
\end{equation}


For $a\neq0$ it gets rather more complicated, because the Jastrow factor from equation \ref{eq:WF_interaction_part} is now different from 1. We also need to add the interaction term $V_{int}$, the hard-sphere potential from equation \ref{eq:V_int}. We are now ready to find an analytical expression for the local energy. By defining
\begin{equation}
f(a, r_{ij})=\exp{\bigg(\sum_{i<j}u(r_{ij})\bigg)}
\end{equation}
and doing a change of variables
\begin{equation}
\frac{\partial}{\partial \vec{r}_k}=\frac{\partial}{\partial \vec{r}_k}\frac{\partial r_{kj}}{\partial r_{kj}}=\frac{\partial r_{kj}}{\partial \vec{r}_k}\frac{\partial}{\partial r_{kj}}=\frac{(\vec{r}_k-\vec{r}_j)}{r_{kj}}\frac{\partial}{\partial r_{kj}}
\end{equation}
one will end up with
\begin{align}
E_L=\sum_k\Bigg(-\frac{1}{2}\bigg(4\alpha^2\Big(x_k^2+y_k^2+\beta^2z_k^2-\frac{1}{\alpha}-\frac{\beta}{2\alpha}\Big)
-4\alpha\sum_{j\neq k}(x_k, y_k, \beta z_k)\frac{(\vec{r}_k-\vec{r}_j)}{r_{kj}}u'(r_{kj})\notag\\
+\sum_{ij\neq k}\frac{(\vec{r}_k-\vec{r}_j)(\vec{r}_k-\vec{r}_i)}{r_{ki}r_{kj}}u'(r_{ki})u'(r_{kj})
+\sum_{j\neq k}\Big(u''(r_{kj})+\frac{2}{r_{kj}}u'(r_{kj})\Big)\bigg)+V_{ext}(\vec{r}_k)\Bigg) + V_{int}.\notag
\label{EL_total}
\end{align}
which is also shown in appendix A. This is not a pretty expression, but will yield correct results for both the interacting and non-inteacting case.



\subsubsection{Numerical calculation of $E_L$} \label{Numerical_calc_E_L}

Another approach when calculating $E_L$ is to split up the local energy expression as shown in equation \ref{eq:EL_num}, and calculate the local energy with a numerical approach where the second derivative needed to find the kinetic energy can be approximated by the three-point formula, see equation \ref{eq:num_derv}.
\begin{equation}
\label{eq:EL_num}
E_{L,i}=-\frac{\hbar^2}{2m}\frac{\nabla_i^2\Psi_T}{\Psi_T}+V_{ext}(\vec{r}_i)=E_{k,i}+E_{p,i}
\end{equation}

\begin{equation}
\label{eq:num_derv}
f''(x)\simeq\frac{f(x+h)-2f(x)+f(x-h)}{h^2}.
\end{equation}
In our case the position is a three dimensional vector, so we need to handle each dimension separately. Both the analytical and the numerical local energies are implemented, and in section \ref{CPU}, the CPU time for the analytical and numerical approach are compared for a various number of particles.
 

\subsubsection{Gross-Pitaevskii equation}
Since we study a dilute bose gas, we can use the Gross-Pitaevskii (GP) equation to estimate the energy density, also when including particle-particle interaction \cite{Gross}, \cite{Pitaevskii}. The energy is then given by an integral over all the positions \cite{Nilsen} as follows:
\begin{equation}
E_{\text{GP}}[\Psi]=\int d\vec{r}\bigg[\frac{\hbar^2}{2m}|\nabla\Psi(\vec{r})|^2+V_{ext}(\vec{r})|\Psi|^2+\frac{2\pi\hbar^2a}{m}|\Psi|^4\bigg].
\label{eq:GP_integral}
\end{equation}
This integral can easily be solved analytically when we ignore the interaction, and for three dimentions we get
\begin{equation}
\label{eq:GP}
E_{\text{GP}}(\alpha, \beta, N)=N\cdot\bigg(1+\frac{\beta}{2}\bigg)\bigg(\frac{1}{4\alpha}+\alpha\bigg)
\end{equation}
 which is derived in appendix D.

This expression is valid for few particles and small hard-sphere diameter. As the average energy per particle  increases as we increase N, the expression is no longer valid for a higher number of particles. For the interacting case, we could solve the integral using a numerical method, for instance Monte Carlo integration. We will not implement this, but rather benchmark our results against the solution of GP when ignoring interaction.


\subsection{Onebody density} \label{sec:OB_theory}
In many cases it is convenient to know the positions of the particles, but when the number of particles increases, the set of positions turns into a messy collection of numbers which is not really informative. Instead of presenting the positions, the density of particles can give us a good overview of where the particles are located. With $N$ particles, the one-body density with respect to a particle $i$ is an integral over all particles but particle $i$:
\begin{equation}
\rho_i=\int_{-\infty}^{\infty}d\vec{r}_1\hdots d\vec{r}_{i-1}d\vec{r}_{i+1}\hdots d\vec{r}_N |\Psi(\vec{r}_1,\hdots \vec{r}_N)|^2.
\end{equation}
For the non-interacting case this integral can be solved analytically, as shown in equation \ref{eq:onebody}. Using Monte Carlo integration, the one-body density can be solved for any case. Anyhow, the interesting part is the radial density, so we either have to solve the integral in spherical coordinates or convert to spherical coordinates afterwards. 

Alternatively, the onebody radial density can be found in a more intuitive way. Imagine we divide the volume around particle $i$ into bins, where bin $j$ is located at a distance $j\cdot r_1$, as shown in figure 1. The radii are thus quantized. By counting the number of particles in a bin and dividing by the surface area, we find the average density of particles in the bin. If we decrease the initial radius $r_1$  of the innermost bin such that we have a large number of bins, this method can be used to find the onebody density. 
\begin{figure}[H]
	\centering
	\begin{tikzpicture}[scale=0.50, thick, dot/.style={shape=circle,inner sep=+0pt, minimum size=+2pt, fill, label={#1}}]
       \coordinate[dot=ri] (ri) at (1,4);
       \coordinate[dot=rj] (rj) at (1,7);

       \foreach \cnt[count=\Cnt] in {.5, 1, 1.5, 2}
         \node[draw, color=red!\Cnt 0!blue, label={[inner sep=+1pt, red!\Cnt 0!blue]below:$ r_{\Cnt} = \Cnt\cdot r_1$}] at (ri) [circle through=($(ri)!\cnt!(rj)$)] {};
	\end{tikzpicture}
	\caption{One can find the onebody density by dividing the volume around a particle with coordinates $\vec{r}_i$ into bins and then count the number of particles, here illustrated in two dimensions.}
\end{figure}

The analytical expression without the Jastrow factor is found from \cite{DuBois}, and can be modified for our particular system, presented for the three dimensional case in equation \ref{eq:onebody}.
\begin{equation}
\rho_i(x_i, y_i, z_i)=\bigg(\frac{2\alpha}{\pi}\bigg)^{3/2}\beta^{1/2}e^{-2\alpha(x_i^2+y_i^2+\beta z_i^2)}.
\label{eq:onebody}
\end{equation}
Since the particles are identical the choice of the particle $i$ is of no consequence. 

\subsection{Scaling} \label{sec:scaling}
For big numerical projects, working with dimensionless quantities is a great advantage. Not only does it improve the code structure and performance, but it also avoids truncation errors due to small constants. For this project a natural scaling parameter for the energy is $\hbar\omega_{ho}$, which appears in the analytical energy expression in equation \ref{eq:Energy_exact}. The equivalent dimensionless equation can then be written as
\begin{equation}
E'=\frac{N\cdot dim}{2}
\end{equation}
where $E'=E/\hbar\omega_{ho}$. Additionally, we can scale the position with respect to the length of the spherical trap, $a_{ho}$, such that 
\begin{equation}
r_i'=\frac{r_i}{a_{ho}}=r_i\cdot\sqrt{\frac{m\omega_{ho}}{\hbar}},
\end{equation}
and the Hamiltonian turns into
\begin{equation}
H=\frac{1}{2}\sum_i\Big(-\nabla^2 + \vec{r}_i^2\Big)+\sum_{i<j}V_{int}(\vec{r}_i,\vec{r}_j)
\label{eq:scaled_hamiltonian}
\end{equation}
A watchful eye will see that this corresponds to setting $\hbar=\omega_{ho}=m=1$, which is the natural units. 

For the spherical trap situation we are left with the variational parameters $\alpha$ and $\beta$ only, but when we study an elliptical trap we still want to get rid of $\omega_z$. Since $\beta^2$ should be the factor in front of the z-coordinate when the Hamiltonian is dimensionless, it can be proven that $\beta=\omega_z/\omega_{ho}$, see appendix C. We end up with the Hamiltonian
\begin{equation}
H=\frac{1}{2}\sum_i\Big(-\nabla^2 + x_i^2 + y_i^2 + \beta^2z_i^2\Big)+\sum_{i<j}V_{int}(\vec{r}_i,\vec{r}_j)
\end{equation} 
where $\beta$ is chosen to be $\sqrt{8}\cong2.82843$, which was used in the experiment of Anderson et.al. \cite{Anderson}.

\subsection{Error estimation} \label{Error_estimation}
When presenting data from an experiment, one should always know the errors in the answer. Experimental data, including data from numerical experiments, are never determined beyond any doubt, and an estimate of this error should therefore be presented alongside the data. \par
There are two kinds of errors. Statistical errors originate from how much statistics one has; when $10^6$ measured points give approximately the same answer, one can be more sure that the actual value is close to those points, more so than if one only has 1 data point. Estimating the statistical error is easily done. The systematic error, however, is harder to handle. It arises for example from calculations being based on faulty theory, or defect measurement devices. Here, we will present how to get an estimate of the statistical error in a numerical experiment.
\par 
\vspace{3mm}
When conducting an experiment $\theta$ with $n$ measured points, $ \{x_1, ..., x_n \}$, the sample mean $ \langle x_{\theta} \rangle$ of the experiment  is defined as shown in equation \ref{eq:sample_mean}.

\begin{equation}
\label{eq:sample_mean}
\langle x_{\theta} \rangle = \frac{1}{n} \sum_{k=1}^n x_{\theta,k}
\end{equation}
The corresponding sample variance $\sigma_{\theta}$ is then defined as

\begin{equation}
\sigma_{\theta}^2 = \frac{1}{n} \sum_{k=1}^n (x_{\theta,k} - \langle x_{\theta} \rangle )^2
\end{equation}
This gives us the error in the given experiment $\theta$. If we repeat this experiment $m$ times, the mean after all the experiments are

\begin{equation}
\label{eq:mean}
\langle x_{m} \rangle = \frac{1}{n} \sum_{k=1}^n  \langle x_{\theta} \rangle.
\end{equation}
The total variance is then

\begin{equation}
\sigma_m^2 = \frac{1}{m} \sum_{\theta=1}^m ( \langle x_{\theta} \rangle - \langle x_{m} \rangle )^2.
\end{equation}
This can be reduced to 

\begin{equation}
\label{eq:sample_var_all_exp}
\sigma_m^2 = \frac{\sigma^2}{n} + \text{covariance term},
\end{equation}
where $\sigma$ is the sample variance over all the experiments, defined as 

\begin{equation}
\sigma^2 = \frac{1}{mn} \sum_{\theta=1}^m \sum_{k=1}^n (x_{\theta, k} - \langle x_m \rangle )^2
\end{equation}
and the covariance is the linear correlation between the measured points. The definition of the covariance is shown in equation \ref{eq:covariance}.

\begin{equation}
\label{eq:covariance}
cov(x,y) = \frac{1}{n^2} \sum_i \sum_{j >i} (x_i - x_j) (y_i - y_j).
\end{equation}

A common simplification is to reduce equation \ref{eq:sample_var_all_exp} to the following:

\begin{equation}
\label{eq:variance_simplified}
\sigma^2 \approx \langle x^2 \rangle - \langle x \rangle^2
\end{equation}

This equation, however, does not take into account the covariance term from equation \ref{eq:sample_var_all_exp}, and as the covariance term is added to the expression for the variance, equation \ref{eq:variance_simplified} will underestimate the uncertainty $\sigma$ for positive covariances.

A direct implementation of equation \ref{eq:sample_var_all_exp} including the covariance term is not suitable, as the expression for the covariance includes a double sum, and for a large number of iterations, this will turn into an extremely time consuming process for a large number of Monte Carlo iterations. Luckily, there are methods for calculating an accurate estimation of the variance without including a double loop in the Monte Carlo program. One of these methods is the blocking method, which is presented in section \ref{Blocking}.

\section{Method} \label{sec:Method}
\subsection{Variational Monte Carlo}\label{VMC}
Variational Monte Carlo (VMC) is a widely used method for approximating the ground state of a quantum system. The method is based on Markov chains, where one particle or a set of particles are moved one step for each cycle, i.e.
\begin{equation}
\vec{R}_{new} = \vec{R} + r\cdot \text{step}.
\end{equation}
Both the direction and particles moves are randomly chosen, so with a plain VMC implementation the particles will move at random, independent from each other. We are going to use the Metropolis algorithm in addition to the VMC, where the Metropolis algorithm accepts or rejects moves based on the probability ratio between the old and the new position. This makes the system approach the most likely state, and the idea is that after a certain number of cycles the system will be in the most likely state. 

\subsection{Metropolis Algorithm} \label{sec:Metropolis}
As mentioned above, the task of the Metropolis algorithm is to move the system towards the most likely state. The standard algorithm, here named brute force, is the simplest one, and does not deal with the transition probabilities. The modified Metropolis-Hastings algorithm includes, on the other hand, the transition probabilities and will be slightly more time consuming per cycle. We expect the latter to converge faster to the most likely state. 

The foundation of the Metropolis algorithm is that the probability for a system to undergo a transition from state $i$ to state $j$ is given by the transition probability multiplied by the acceptance probability
\begin{equation}
W_{i\rightarrow j} = T_{i \rightarrow j}\cdot A_{i \rightarrow j}
\end{equation}
where $T_{i \rightarrow j}$ is the transition probability and $A_{i \rightarrow j}$ is the acceptance probability. Built on this, the probability for being in a state $i$ at time (step) $n$ is
\begin{equation}
P_i^{(n)} = \sum_j\bigg[P_j^{(n-1)}T_{j \rightarrow i}A_{j \rightarrow i} + P_i^{(n-1)}T_{i \rightarrow j}(1-A_{i \rightarrow j})\bigg]
\end{equation}
since this can happen in two different ways. One can start in this state $i$ at time $n-1$ and be rejected or one can start in another state $j$ at time $n-1$ and complete an accepted move to state $i$. In fact $\sum_j T_{i \rightarrow j} =1$, so we can rewrite this as 
\begin{equation}
P_i^{(n)} = P_i^{(n-1)} + \sum_j\bigg[P_j^{(n-1)}T_{j \rightarrow i}A_{j \rightarrow i} - P_i^{(n-1)}T_{i \rightarrow j}A_{i \rightarrow j}\bigg].
\end{equation}
When the times goes to infinity, the system approaches the most likely state and we will have $P_i^{(n)} = p_i$, which requires
\begin{equation}
\sum_j\bigg[p_jT_{j \rightarrow i}A_{j \rightarrow i} - p_iT_{i \rightarrow j}A_{i \rightarrow j}\bigg]=0.
\end{equation}
Rearranging, we obtain the useful result
\begin{equation}
\frac{A_{j\rightarrow i}}{A_{i\rightarrow j}}=\frac{p_iT_{i\rightarrow j}}{p_jT_{j\rightarrow i}}
\end{equation}
which will be used in the acceptance criteria.

\subsubsection{Brute force}
In the brute force Metropolis algorithm we want to check if the new position is more likely than the current position, and for that we calculate the probabilities $P(\vec{R})=|\Psi_T(\vec{R})|^2$ for both positions. We get rid off the transition probabilities setting $T_{i\rightarrow j}=T_{j\rightarrow i}$, and then end up with the plain ratio
\begin{equation}
w=\frac{P(\vec{R}_{new})}{P(\vec{R})}=\frac{|\Psi_T(\vec{R}_{new})|^2}{|\Psi_T(\vec{R})|^2}.
\end{equation}
$w$ will be larger than one if the new position is more likely than the current, and smaller than one if the current position is more likely than the new one. Metropolis handle this by accepting the move if the ratio $w$ is larger than a random number $r$ in the interval $[0,1]$, and rejecting if not:
\begin{equation}
\text{New position: }
\begin{cases} 
   \text{accept} & \text{if}\quad w > r \\
   \text{reject} & \text{if}\quad w \leq r.
\end{cases}
\end{equation}

\subsubsection{Importance sampling} \label{Importance_sampling}
The importance sampling technique is often refered to as Metropolis-Hastings algorithm. The approach is the same as for the brute force Metropolis algorithm, but we will end ut with a slightly more complicated acceptation criteria. To understand the details, we need to begin with the Fokker-Planck equation, which describes the time-evolution of the probability density function $P(R,t)$. In one dimension it reads
\begin{equation}
\frac{\partial P(R,t)}{\partial t} = D\frac{\partial}{\partial R}\bigg(\frac{\partial}{\partial R} - F\bigg)P(R,t).
\end{equation}
where $F$ is the drift force given by equation \ref{eq:drift_force} and $D$ is the diffusion coefficient, in this case equal to $0.5$. Calculations of the analytical expression of the drift force $F$ for a spherical harmonic oscillator and $a=0$ can be found in appendix B.

\begin{equation}
	\label{eq:drift_force}
	F(R) = \frac{2 \nabla \psi_T}{\psi_T}
\end{equation}
Even though the probability density function can give a lot of useful information, an equation describing the motion of such a particle would be more appropriate for our purposes. Fortunately this equation exists, and satisfies the Fokker-Planck equation. The Langevin equation can be written as
\begin{equation}
\frac{\partial R(t)}{\partial t}=DF(R(t)) + \eta
\end{equation}
where $\eta$ can be considered as a random variable. This differential equation can be solved by applying the forward Euler method and introducing gaussian variables $\xi$
\begin{equation}
R_{new} = R + DF(R)\Delta t + \xi\sqrt{\Delta t}
\end{equation}
which will be used to update the position. This is an improved way of choosing the direction in which the particle is moved compared to the brute force algorithm, as the drift force $F(R)$ says something about which direction the particle is pushed in, and the choice of the new proposed position is thus dependent on this.

Moreover we also need to update the acceptance criteria since we no longer ignore the transition probabilities. With the Fokker-Planck equation as base, the transition probabilities are given by Green's function
\begin{align}
G(R_{new},R,\Delta t)&=\frac{1}{(4\pi D\Delta t)^{3N/2}}\exp[-(R_{new} - R - D\Delta tF(R))^2/4D\Delta t]\notag\\
&=T_{R\rightarrow R_{new}}
\end{align}
and the acceptance criteria becomes
\begin{equation}
r<\frac{G(R,R_{new},\Delta t)|\Psi_T(R_{new})|^2}{G(R_{new},R,\Delta t)|\Psi_T(R)|^2}.
\end{equation}



\subsection{Minimization methods}
When the interaction term is excluded, we know which $\alpha$ that corresponds to the energy minimum, and it is in principle no need to try different $\alpha$'s. However, sometimes we have no idea where to search for the minimum point, and we need to try various $\alpha$ values to determine the lowest energy. If we do not know where to start searching, this can be a time consuming activity. Would it not be nice if the program could do this for us?

In fact there are multiple techniques for doing this, where the most complicated ones obviously also are the best. Anyway, in this project we will have good initial guesses, and are therefore not in need for the most fancy algorithms. 

\subsubsection{Gradient descent}
Perhaps the simplest and most intuitive method for finding the minimum is the gradient descent method (GD), which reads
\begin{equation}
\label{eq:GD}
\alpha^+=\alpha - \eta\cdot\frac{d\langle E(\alpha)\rangle}{d\alpha}.
\end{equation}
where $\alpha^+$ is the updated $\alpha$ and $\eta$ is a step size. The idea is that one finds the gradient of the energy with respect to a certain $\alpha$, and moves in the direction which minimizes the energy. This is repeated until one has found an energy minimum, where the energy minimum is defined as either where $\frac{d\langle E(\alpha)\rangle}{d\alpha}$ is smaller than a given tolerance, or the energy fluctuates around a value are smaller than a tolerance, and thus changes minimally.
\par 
\vspace{3mm}

To implement equation \ref{eq:GD}, we need an expression for the derivative of $E$ with respect to alpha:

\begin{equation}
	\label{eq:E_L_der_wrt_alpha}
	\bar{E_{\alpha}} = \frac{d \langle E (\alpha) \rangle}{d \alpha}.
\end{equation}
By using the expression for the expectation value for the energy $ \langle E (\alpha) \rangle$ in equation \ref{eq:exp_EL} 

\begin{equation}
	\label{eq:exp_EL}
	\langle E (\alpha) \rangle = \frac{ \langle \psi_T(\alpha) | H | \psi_T(\alpha)  \rangle}{ \langle \psi_T(\alpha)  |  \psi_T(\alpha)  \rangle }
\end{equation}
and applying the chain rule of differentiation, it can be shown that equation \ref{eq:E_L_der_wrt_alpha} is equal to equation \ref{eq:E_L_der_wrt_alpha_expression}

\begin{equation}
	\label{eq:E_L_der_wrt_alpha_expression}
	\bar{E_{\alpha}} = 2 \bigg[\langle E_L (\alpha)  \frac{ \bar{\psi_{\alpha}}}{\psi_{\alpha}}\rangle - \langle E_L (\alpha) \rangle \langle \frac{\bar{\psi_{\alpha}}}{\psi_{\alpha}} \rangle\bigg]
\end{equation}
where

\begin{equation}
	\bar{\psi_{\alpha}} = \frac{d \psi (\alpha)}{d \alpha}.
\end{equation}

The algorithm of this minimization method is thus as follows:

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}

for(max number of iterations with minimizing)
	
	do M Monte Carlo cycles
	calculate E and dE/dalpha 
	
	Check if dE/dalpha < eps or alpha fluctuation over the last 5 steps is < eps
	
	
		if yes, print optimal alpha and break loop
		if no, continue to next iteration


\end{lstlisting}

\subsection{Blocking method} \label{Blocking}

As described in section \ref{Error_estimation}, we need a method to give a proper estimation of the variance $\sigma^2$ of the points in our experiment, preferably without calculating the double loop from the expression of the covariance in equation \ref{eq:covariance}. 
\par 
\vspace{3mm}

One method that can be used, is the blocking method, which is quite fast and can handle large data sets. Say that we have a data set $\{x_1, ...,x_i,..., x_n\}$ from an experiment, which in our case will be the estimations of the local energies for each Monte Carlo cycle. The mean of this data set is $m$, and we want to estimate the variance of this data set, $\sigma^2 (m)$, including the covariance.

The variance is defined as 

\begin{equation}
	\sigma^2 = \frac{1}{n-1} \sum_i (x_i - m)
\end{equation}
However, this does not include the covariance, as the points $x_i$ are correlated.

If we transform the data set $\{x_1, ...,x_i,..., x_n\}$ by taking the mean of two neighbouring points in the following way
\begin{equation}
	x'_i = \frac{1}{2}(x_{2i} + x_{2i+1})
\end{equation}
the number of points in the transformed data set $n' = \frac{1}{2} n$. Each pair of points $x'_i$ is referred to as a block, and using the variance formula above, the variance within each block is calculated. An estimate of the total variance $\hat{\sigma}$ is then
 
 \begin{equation}
 \hat{\sigma} = \frac{\sigma_1 + ... + \sigma_{n'}}{n'}
 \end{equation}
 
By repeating this procedure several times, and each time using more measurements for each block, the estimated standard deviation $\hat{\sigma}$ will grow, and eventually reach a plateau. When the estimated standard deviation $\hat{\sigma}$ flats out, it means that the blocks are no longer correlated, and thus the covariance which bugged us in equation \ref{eq:sample_var_all_exp}, is now 0. The covariance is therefore accounted for, and the $\hat{\sigma}$ is now a good estimation of the error in the data set.


\section{Code} \label{sec:code}
The focus of this project is developing a code to make VMC run correctly, and in this section we will explain briefly how the code works and which implementation techniques we have used. All quantities used in the implementation are dimensionless. 

\subsection{Code structure}
To keep the program neat, specific parts were placed in specific files. The file wavefunction.cpp contains the class Wavefunction, which both sets up the wave function and calculate the local energy given a wave function. The file metropolis.cpp contains the Variational Monte Carlo loop, and the file gd.cpp handles the calculation of the optimal variational parameter $\alpha$. We also collected the tools that are used by multiple scripts in its own file to maximize the reuse of the code, in particular Green's function and the quantum force calculations. The user only has to deal with main.cpp, as all choices of parameters are to be specified here.  In figure \ref{fig:code_structure} we give an overview of the structure, where the lines indicate which scripts that communicate. 

\begin{figure}[H]
\centering
\begin{tikzpicture}[auto,node distance=1.5cm]
  \node[entity] (node1) {VMC}
    child {};
  \node[entity] (node2) [below right = of node1] {WF};
  \node[entity] (node3) [below left = of node1] {Main};
  \node[entity] (node4) [below right = of node3] {GD};
  \node[relationship] (rel1) [below=1cm of node1] {Tools};

  \path (node1) edge node {} (node2)
                edge node {} (node3)
                edge node {} (rel1);
  \path (node4) edge node {} (node2)
                edge node {} (node3)
                edge node {} (rel1);
\end{tikzpicture}
\caption{An overview of the code structure. metropolis.cpp is refered to as VMC, gd.cpp is GD, wavefunction.cpp is WF, tools.cpp is Tools and obviously main.cpp is Main. See text for furter description.}
\label{fig:code_structure}
\end{figure}
 A standard VMC run goes as follows: the desired simulation case is set up in main.cpp (Main) by specifying the given variables. metropolis.cpp (VMC) is called from Main, which calls wavefunction.cpp (WF) to set up the class WaveFunction. The Monte Carlo loop begins, where the move of one random particle is proposed for each iteration. Depending of whether brute force or importance sampling have been chosen, either WF or tools.cpp (Tools) is called to obtain the transition probability, and the move is either accepted or rejected. VMC calls WF to obtain the local energy of the new position. After M iterations, the estimation of the energy is printed. \par
 When running the gradient decent method, then Main calls gd.cpp (GD). A start value for $\alpha$ is given in the program, and the Monte Carlo loop is conducted as described above. After the loop, the estimated $E$-value is printed. Based on the parameter $\frac{d E}{d \alpha}$, a new value for $\alpha$ is chosen. After several MC loops for different $\alpha$, the optimal $\alpha$ is chosen. 
 
 \subsubsection{Test}
To make sure the program behaves as expected, we have implemented tests. All tests are functions, and can be found in the test.cpp file.

The test "test EL" checks the output estimated energy after the Monte Carlo loops for a given $\alpha$ in the no interaction case, and compares it to the analytical energy obtrained from the Gross-Pitaevskii equation from equation \ref{eq:GP}. If the difference between the two are larger than a given tolerance, an error is raised.

\subsection{Implementation}
We use the vector package to create a 2D-array where the positions are stored. For the exact implementation, see the github repository linked on page one. We always struggle for better performance, and to achieve that we try to avoid heavy loops and repetition of calculations. A second priority is to make the code as general as possible, such that the code can be reused for other purposes. As an example we always loop over an arbitrary given number of dimensions, and similar operations are done throughout the programs. 

\subsubsection{VMC}

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}
Set up position matrix of N paricles in dim dimentions
Initialize class WaveFunction
	
for M Monte Carlo iterations
	move random particle
	accept or reject move based on transition probability
	calculate energy of new position
	update sum of E_L

print estimated E 
	
\end{lstlisting}

\subsubsection{GD}

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}

first choice of alpha, number of gradientdecent-iteartions T and tolerance is specified

for T iterations 
	for M Monte Carlo cycles
		move random particle
		accept or reject move based on transition probability
		calculate energy of new position
		update sum of E_L and dE/dalpha 
		
	print E and dE/dalpha
	
	test if optimal alpha -> if optimal, break loop and print optimal alpha
	
	based on dE/d alpha, suggest new alpha	

\end{lstlisting}



\section{Results} \label{sec:Results}


\section{Discussion} \label{sec:Discussion}
 

\section{Conclusion} \label{sec:Conclusion}



\newpage

\section{Appendix A} \label{sec:appendix_A}

\newpage
\section{References}
\begingroup
\renewcommand{\section}[2]{}
\begin{thebibliography}{}
	\bibitem{MHJ15}
	Morten Hjorth-Jensen.
	Computational Physics 2: Variational Monte Carlo methods, Lecture Notes Spring 2018.
	Department of Physics, University of Oslo,
	(2018).
	\bibitem{DuBois}
	J. L. DuBois and H. R. Glyde, H. R., \emph{Bose-Einstein condensation in trapped bosons: A variational Monte Carlo analysis}, Phys. Rev. A \textbf{63}, 023602 (2001).
	\bibitem{Nilsen}
	J. K. Nilsen,  J. Mur-Petit, M. Guilleumas, M. Hjorth-Jensen, and A. Polls, \emph{Vortices in atomic Bose-Einstein condensates in the large-gas-parameter region}, Phys. Rev. A \textbf{71}, 053610, (2005).
	\bibitem{Dalfovo}
	F. Dalfovo, S. Stringari, \emph{Bosons in anisotropic traps: ground state and vortices} Phys. Rev. A \textbf{53}, 2477, (1996).
	\bibitem{JE2016}
	J. Emspak, \emph{States of Matter: Bose-Einstein Condensate}, LiveScience, (2016).
	\url{https://www.livescience.com/54667-bose-einstein-condensate.html}
	Downloaded March 15th 2018.
	\bibitem{SP}
	S. Perkowitz \emph{Bose-Einstein condensate} Encyclopaedia Britannica 
	\url{https://www.britannica.com/science/Bose-Einstein-condensate}
	Downloaded March 15th 2018.
	\bibitem{Anderson}
	M. H. Anderson, J. R. Ensher, M. R. Matthews, C. E. Wieman, E. A. Cornell, \emph{Observation of Bose-Einstein Condensation in a Dilute Atomic Vapor}, Science \textbf{269}, (1995).
	\bibitem{JKNilsen}
	J. K. Nilsen \emph{Bose-Einstein condensation in trapped bosons: A quantum Monte Carlo analysis}, Master thesis 2004, Department of Physics, University of Oslo, (2004). 
	\bibitem{Gross}
	E. P. Gross, \emph{Structure of a quantized vortex in boson systems}, Il Nuovo Cimento, \textbf{20} (3): 454–457, (1961).
	\bibitem{Pitaevskii}
	L. P. Pitaevskii, \emph{Vortex lines in an imperfect Bose gas}, Sov. Phys. JETP. \textbf{13} (2): 451–454, (1961).
	
	
\end{thebibliography}
\endgroup

\end{document}
