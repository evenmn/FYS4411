\documentclass[norsk,a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx} %for å inkludere grafikk
\usepackage{verbatim} %for å inkludere filer med tegn LaTeX ikke liker
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{float}
\usepackage{color}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{tikz}

\lstset{language=c++}
\lstset{basicstyle=\small}
\lstset{backgroundcolor=\color{white}}
\lstset{frame=single}
\lstset{stringstyle=\ttfamily}
\lstset{keywordstyle=\color{red}\bfseries}
\lstset{commentstyle=\itshape\color{blue}}
\lstset{showspaces=false}
\lstset{showstringspaces=false}
\lstset{showtabs=false}
\lstset{breaklines}
\lstset{postbreak=\raisebox{0ex}[0ex][0ex]{\ensuremath{\color{red}\hookrightarrow\space}}}
\usepackage{titlesec}

\setcounter{secnumdepth}{4}
\usetikzlibrary{through,calc}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}


\title{FYS4411 - Computational Physics II\\\vspace{2mm} \Large{Project 1}}
\author{\large Dorthea Gjestvang\\ Even Marius Nordhagen}
\date\today
\begin{document}

\maketitle
\begin{abstract}
This project aims to simulate a trapped Bose gas using Variational Monte Carlo (VMC) with both the  brute force Metropolis and the Metropolis-Hastings algorithms. Using a trial wave function with spherical and elliptical harmonic oscillator (HO) potential, with and without a hard sphere potential, we calcuate the ground state energy and onebody density for different numbers of particles. Furthermore, the results are benchmarked with earlier publications. \par 

Using the spherical HO with no interaction, both the brute force and the Hastings Metropolis methods reproduces analytical calculations. Though the Hastings algorithm is slightly slower, it has a higher acceptance rate for the proposed moves in the VMC simulation. The onebody density drops when introducing the hard sphere potential. The system behaves as expected, and our results reproduces the benchmarks.
\par 

\end{abstract}


\begin{itemize}
\item Github repository containing programs and results are in: \url{https://github.com/evenmn/FYS4411/tree/master/Project%201}
\end{itemize}


\section{Introduction}

In this project, we study the  phenomen Bose-Einstein condensation, a highly relevant subject in modern physics.  Bose-Einstein condensation occurs when atoms are cooled towards absolute zero, as they start occupying the same quatum states \cite{JE2016}. Even though the effect was predicted in 1924, the effect was first demonstrated decades later \cite{SP}.

Our goal in this project is to simulate a trapped, hard sphere Bose gas for different numbers of particles for a given trial wave function, and to evaluate the ground state energy of this system. The system is evaluated both for a sperical and elliptical harmonic oscillator trap. The model is descriped in section \ref{Theory}. \par

Using Variational Monte Carlo method with two different Metropolis algorithms, presented in section \ref{Method}, we use the trial wave function to calculate the ground state energy and the onebody density of the system. The results are presented in section \ref{Results}. In section \ref{Discussion} we discuss our results, and compare them to results obtained from the article found at (s \cite{Nilsen} and) \cite{DuBois}.


\section{Theory} \label{Theory}

\subsection{Presentation of potential and trial wavefunction} 

We study a system of $N$ bosons trapped in a harmonic oscillator with the Hamiltonian given by 
\begin{equation}
\hat{H}=\sum_i^N\bigg(-\frac{\hbar^2}{2m}\nabla_i^2+V_{ext}(\vec{r}_i)\bigg)+\sum_{i<j}^NV_{int}(\vec{r}_i,\vec{r}_j)
\label{eq:Hamilton}
\end{equation}
with $V_{ext}$ as the external potential, which is the harmonic oscillator potential,
and $V_{int}$ as the interaction term, defined in equation \ref{eq:V_int}, which ensures the particles to be separated by a distance $a$ . We will consider a harmonic oscillator which can either be spherical (all dimensions have the same scales) or elliptical (the vertical dimension has a different frequency from the horizontals),
\begin{equation}
\label{eq:V_ext}
V_{ext}(\vec{r})=
\begin{cases} 
   \frac{1}{2}m\omega_{HO}^2\vec{r}^2 & \text{(Spherical)} \\
   \frac{1}{2}m[\omega_{HO}^2(x^2 + y^2) + \omega_z^2z^2] & \text{(Elliptical)}.
\end{cases}
\end{equation}

\begin{equation}
\label{eq:V_int}
V_{int}(\vec{r}_i, \vec{r}_j)=
\begin{cases} 
0 & \text{if}\quad |\vec{r}_i-\vec{r}_j| \geq a \\
\infty & \text{if}\quad |\vec{r}_i-\vec{r}_j| < a
\end{cases}
\end{equation}

The trial wavefunction is on the form 
\begin{equation}
\Psi_T(\vec{r}_1, \vec{r}_2, ..., \vec{r}_N, \alpha, \beta)=\prod_i^Ng(\alpha, \beta, \vec{r}_i)\prod_{i<j}f(a,r_{ij})
\label{eq:WF}
\end{equation}
where $r_{ij}=|\vec{r}_i-\vec{r}_j|$ and $g$ is assumed to be an exponential function,
\begin{equation}
g(\alpha, \beta, \vec{r}_i)=\exp[-\alpha(x_i^2+y_i^2+\beta z_i^2)],
\end{equation}
which is practical since
\begin{equation}
\prod_i^Ng(\alpha, \beta, \vec{r}_i)=\exp\Big[-\alpha\sum_{i=1}^N(x_i^2+y_i^2+\beta z_i^2)\Big].
\end{equation}
$\alpha$ is a variational parameter that we later use to find the energy minimum, and $\beta$ is a constant. This is also the form of the exact ground state wave function for a harmonic oscillator, so by choosing the correct $\alpha$, we will find the exact ground state energy (when $V_{int}$ is ignored). The $f$ presented above is the correlation wave function, which is 
\begin{equation}
\label{eq:WF_interaction_part}
f(a,r_{ij})=
\begin{cases} 
   0 & r_{ij} \leq a \\
   \left(1-\frac{a}{r_{ij}}\right) & r_{ij} > a.
\end{cases}
\end{equation}

where $a$ is a parameter describing the minimum distance allowed between particles in the trap.

\subsection{Local energy $E_L$ calculation}

We want to calculate the local energy as a function of $\alpha$ for the trial wave function given in equation \ref{eq:WF}, and then use Variational Monte Carlo (VMC) described in section \ref{VMC}. 

We obtain the general  expression for $E_L$ by rewriting Schr\"{o}dinger's equation, as shown in \ref{eq:Local_energy}.

\begin{equation}
E_L(\vec{r})=\frac{1}{\Psi_T(\vec{r})}\hat{H}\Psi_T(\vec{r}).
\label{eq:Local_energy}
\end{equation}

When the repulsive interaction is ignored ($a=0$), it can be shown that the local energy for a system of $N$ particles and $dim$ free dimensions is given by
\begin{equation}
E_L=dim\cdot N\cdot \alpha + \Big(\frac{1}{2}-2\alpha^2\Big)\sum_i\vec{r}_i^2,
\end{equation}
which is proven in Appendix A. This is only true for the a spherical harmonic oscillator trap, for the elliptical trap we need to add $\beta$ in front of the z-component. You may also notice that this equation is scaled, more about that in section \ref{sec:scaling}.

For the a spherical harmonic oscillator where particle interactions are ignored, the analytical expression for one particle in a harmonic oscillator is well-known and reads $E = \hbar\omega(n + dim/2)$ where $n$ is the energy level and $dim$ is number of dimensions. In this project we will study the ground state only, such that $n=0$, and for $N$ particles and $dim$ free dimensions we therefore obtain the expression for the ground state $E_L$ shown in equation \ref{eq:Energy_exact}.

\begin{equation}
E = \frac{1}{2}N\cdot dim\cdot\hbar\omega_{HO}.
\label{eq:Energy_exact}
\end{equation}


For $a\neq0$ it gets rather more complicated, because the correlation term from equation \ref{eq:WF_interaction_part} is now different from 1. We also need to add the interaction term, set to a so-called hard-sphere potential from equation \ref{eq:V_int}. We are now ready to find a general expression for the local energy. By defining
\begin{equation}
f(a, r_{ij})=\exp{\bigg(\sum_{i<j}u(r_{ij})\bigg)}
\end{equation}
and doing a change of variables
\begin{equation}
\frac{\partial}{\partial \vec{r}_k}=\frac{\partial}{\partial \vec{r}_k}\frac{\partial r_{kj}}{\partial r_{kj}}=\frac{\partial r_{kj}}{\partial \vec{r}_k}\frac{\partial}{\partial r_{kj}}=\frac{(\vec{r}_k-\vec{r}_j)}{r_{kj}}\frac{\partial}{\partial r_{kj}}
\end{equation}
one will end up with
\begin{align}
E_L=\sum_k\Bigg(-\frac{1}{2}\bigg(4\alpha^2\Big(x_k^2+y_k^2+\beta^2z_k^2-\frac{1}{\alpha}-\frac{\beta}{2\alpha}\Big)
-4\alpha\sum_{j\neq k}(x_k, y_k, \beta z_k)\frac{(\vec{r}_k-\vec{r}_j)}{r_{kj}}u'(r_{kj})\notag\\
+\sum_{ij\neq k}\frac{(\vec{r}_k-\vec{r}_j)(\vec{r}_k-\vec{r}_i)}{r_{ki}r_{kj}}u'(r_{ki})u'(r_{kj})
+\sum_{j\neq k}\Big(u''(r_{kj})+\frac{2}{r_{kj}}u'(r_{kj})\Big)\bigg)+V_{ext}(\vec{r}_k)\Bigg) + V_{int}.\notag
\label{EL_total}
\end{align}
This is not a pretty expression, but hopefully it will give us the correct answers. 



\subsubsection{Numerical calculation of $E_L$} \label{Numerical_calc_E_L}

Another approach when calculating $E_L$ is to split up the local energy expression as shown in equation \ref{eq:EL_num}, and calculate the local energy with a numerical approach where the second derivative can be approximated by the three-point formula, shown in equation \ref{eq:num_derv}
\begin{equation}
\label{eq:EL_num}
E_{L,i}=-\frac{\hbar^2}{2m}\frac{\nabla_i^2\Psi_T}{\Psi_T}+V_{ext}(\vec{r}_i)=E_{k,i}+E_{p,i}
\end{equation}

\begin{equation}
\label{eq:num_derv}
f''(x)\simeq\frac{f(x+h)-2f(x)+f(x-h)}{h^2}.
\end{equation}
In our case the position is a three dimensional vector, so we need to handle each dimension separately. Both the analytical and the numerical local energy are implemented, and in section \ref{CPU}, the CPU time for the analytical and numerical approach are compared for a various number of particles. 


\subsection{Onebody density} \label{sec:OB_theory}
In many cases it is convenient to know the positions of the particles, but when the number of particles increases, the set of positions turns into a messy collection of numbers which is not really informative (and in fact the exact positions cannot be revealed according to the uncertainity principle). Instead of presenting the positions, the density of particles can give us a good overview of where the particles can be found. With $N$ particles, the one-body density with respect to a particle $i$ is an integral over all particles but particle $i$
\begin{equation}
\rho_i=\int_{-\infty}^{\infty}d\vec{r}_1\hdots d\vec{r}_{i-1}d\vec{r}_{i+1}\hdots d\vec{r}_N |\Psi(\vec{r}_1,\hdots \vec{r}_N)|^2.
\end{equation}
For the non-interacting case this integral can be solved analytically, or we can use Monte Carlo integration to solve it for any case. Anyhow, the interesting part is the radial density, so we either have to solve the integral in spherical coordinates or convert to spherical coordinates afterwards. 

Alternatively, the onebody radial density can be found in a more intuitive way. Imagine we divide the volume around particle $i$ into bins, where bin $j$ is located at a distance $j\cdot r_1$ (the radii are quantized). By counting the number of particles in a bin and dividing on the surface area, we find the average density of the bin. If we further decrease the initial radius $r_1$ (radius of the innermost bin) such that we have a large number of bins, this method can be used to find the onebody density. 
\begin{figure}[H]
	\centering
	\begin{tikzpicture}[scale=0.50, thick, dot/.style={shape=circle,inner sep=+0pt, minimum size=+2pt, fill, label={#1}}]
       \coordinate[dot=r] (r) at (1,4);
       \coordinate[dot=rj] (rj) at (1,7);

       \foreach \cnt[count=\Cnt] in {.5, 1, 1.5, 2}
         \node[draw, color=red!\Cnt 0!blue, label={[inner sep=+1pt, red!\Cnt 0!blue]below:$ r_{\Cnt} = \Cnt\cdot r_1$}] at (r) [circle through=($(r)!\cnt!(rj)$)] {};
	\end{tikzpicture}
	\caption{One can find the onebody densiy by dividing the volume around a particle with coordinates $\vec{r}_i$ into bins and then count the number of particles, here illustrated in two dimensions.}
\end{figure}


\subsection{Scaling} \label{sec:scaling}
For big numerical projects, working with dimensionless quantities is a great advantage. Not only does it improve the code structure and performance, but it also avoids truncation errors due to small constants. For this project a natural scaling parameter for the energy is $\hbar\omega_{HO}$, which appears in the analytical energy expression in equation (\ref{eq:Energy_exact}). The equivalent dimensionless equation can then be written as
\begin{equation}
E'=\frac{N\cdot dim}{2}
\end{equation}
where $E'=E/\hbar\omega_{HO}$. Additionally we can scale the position with respect to the length of the spherical trap, $a_{HO}$, such that 
\begin{equation}
r_i'=\frac{r_i}{a_{HO}}=r_i\cdot\sqrt{\frac{m\omega_{HO}}{\hbar}},
\end{equation}
and the Hamiltonian turns into
\begin{equation}
H=\frac{1}{2}\sum_i\Big(-\nabla^2 + \vec{r}_i^2\Big)+\sum_{i<j}V_{int}(\vec{r}_i,\vec{r}_j)
\end{equation}
A watchful eye will see that this corresponds to setting $\hbar=\omega_{HO}=m=1$, which is the natural units. 

For the spherical trap situation we are left with the variational parameters $\alpha$ and $\beta$ only, but when we study an elliptical trap we still want to get rid of $\omega_z$. Since $\beta^2$ should be the factor in front of the z-coordinate when the Hamiltonian is dimensionless, it can be proven that $\beta=\omega_z/\omega_{HO}$, see appendix C. We end up with the Hamiltonian
\begin{equation}
H=\sum_i\bigg(\frac{1}{2}\Big(-\nabla^2 + x_i^2 + y_i^2 + \beta^2z_i^2\Big)\bigg)+\sum_{i<j}V_{int}(\vec{r}_i,\vec{r}_j)
\end{equation} 
where $\beta$ is chosen to be 2.82843 due to experimental results.

\subsection{Error estimation} \label{Error_estimation}
When presenting data from an experiment, one should always know the errors in the answer. Experimental data, including data from numerical experiments, are never determined beyond any doubt, and an estimate of this error should therefore be presented alongside the data. 
There are two kinds of errors. Statistical errors originate from how much statistics one has; when $10^6$ measured points give approximately the same answer, one can be more sure that the actual value is close to those points, more than if one only has 1 point of statistical data. Estimating the statistical error is easily done. The systematic error, however, is harder to handle. It arises for example from calculations being based on faulty theory, or defect measurement devices. Here, we will present how to get an estimate of the statistical error in a numerical experiment.
\par 
\vspace{3mm}
When conducting an experiment $\alpha$ with $n$ measured points,$x_n$, the sample mean of the experiment $\langle x \rangle$ is defined as shown in equation \ref{eq:sample_mean}.

\begin{equation}
\label{eq:sample_mean}
\langle x_{\alpha} \rangle = \frac{1}{n} \sum_{k=1}^n x_{\alpha,k}
\end{equation}

The corresonding sample variance $\sigma_{\alpha}$ is then defined as

\begin{equation}
\sigma_{\alpha}^2 = \frac{1}{n} \sum_{k=1}^n (x_{\alpha,k} - \langle x_{\alpha} \rangle )^2
\end{equation}

This gives us the error in the given experiment $\alpha$. If we repeat this experiment $m$ times, the mean after all the experiments are

\begin{equation}
\label{eq:mean}
\langle x_{m} \rangle = \frac{1}{n} \sum_{k=1}^n  \langle x_{\alpha} \rangle
\end{equation}

The total variance is then

\begin{equation}
\sigma_m^2 = \frac{1}{m} \sum_{\alpha=1}^m ( \langle x_{\alpha} \rangle - \langle x_{m} \rangle )^2
\end{equation}

This can be reduced to 

\begin{equation}
\label{eq:sample_var_all_exp}
\sigma_m^2 = \frac{\sigma^2}{n} + covariance term
\end{equation}

where $\sigma$ is the sample variance over all the experiments, defined as 

\begin{equation}
\sigma^2 = \frac{1}{mn} \sum_{\alpha=1}^m \sum_{k=1}^n (x_{\alpha, k} - \langle x_m \rangle )^2
\end{equation}

and the covariance is the linear correlation between the measured points. The definition of the covariance is shown in equation \ref{eq:covariance}.

\begin{equation}
\label{eq:covariance}
cov(x,y) = \frac{1}{n^2} \sum_i \sum_{j >i} (x_i - x_j) (y_i - y_j)
\end{equation}

A common simplification is to reduce equation \ref{eq:sample_var_all_exp} to the following:

\begin{equation}
\label{eq:variance_simplified}
\sigma^2 \approx \langle x^2 \rangle - \langle x \rangle^2
\end{equation}

This equation, however, does not take into account the covariance term from equation \ref{eq:sample_var_all_exp}, and as the covariance term is added to the expression for the variance, \ref{eq:variance_simplified} will underestimate the uncertainty $\sigma$ for positive covariances.

A direct implementation of equation \ref{eq:sample_var_all_exp} including the covariance term is not suitable, as the expresion for the covariance includes a double sum, and for a large number of iterations, this will turn into an extremely time-consuming process for a large number of Monte Carlo iterations. Luckily, there are methods for calculating an accurate estimation of the variance without including a double loop in the Monte Carlo program. One of these methods is the blocking method, which is presented in section \ref{Blocking}.

\section{Method} \label{Method}
\subsection{Variational Monte Carlo}\label{VMC}
Variational Monte Carlo (VMC) is a widely used method for approximating the ground state of a quantum system. The method is based on Markov chains, and move a particle (or a set of particles) one step for each cycle, i.e.
\begin{equation}
\vec{R}_{new} = \vec{R} + r\cdot \text{step}.
\end{equation}
Both the direction and the change in position are randomly chosen, so with a plain VMC implementation the particles will move randomly and independently of each other. We are going to use the Metropolis algorithm in addition to the VMC, which accepts or rejects moves based on the probability ratio between the old and the new position. This makes the system approach the most likely state, and the idea is that after a certain number of cycles the system will be in the most likely state. 

\subsection{Metropolis Algorithm}
As mentioned above the task of the Metropolis algorithm is to move the system against the most likely state. The standard algorithm, here named brute force, is the simplest one, and does not deal with the transition probabilities. The modified Metropolis-Hastings algorithm includes, on the other hand, the transition probabilities and will be slightly more time consuming per cycle. We expect the latter to converge faster to the most likely state. 

The foundation of the Metropolis algorithm is that the probability for a system to undergo a transition from state $i$ to state $j$ is given by the transition probability multiplied by the acceptance probability
\begin{equation}
W_{i\rightarrow j} = T_{i \rightarrow j}\cdot A_{i \rightarrow j}
\end{equation}
where $T_{i \rightarrow j}$ is the transition probability and $A_{i \rightarrow j}$ is the acceptance probability. Built on this, the probability for being in a state $i$ at time (step) $n$ is
\begin{equation}
P_i^{(n)} = \sum_j\bigg[P_j^{(n-1)}T_{j \rightarrow i}A_{j \rightarrow i} + P_i^{(n-1)}T_{i \rightarrow j}(1-A_{i \rightarrow j})\bigg]
\end{equation}
since this can happen in two different ways. One can start in this state $i$ at time $n-1$ and be rejected or one can start in another state $j$ at time $n-1$ and complete an accepted move to state $i$. In fact $\sum_j T_{i \rightarrow j} =1$, so we can rewrite this as 
\begin{equation}
P_i^{(n)} = P_i^{(n-1)} + \sum_j\bigg[P_j^{(n-1)}T_{j \rightarrow i}A_{j \rightarrow i} - P_i^{(n-1)}T_{i \rightarrow j}A_{i \rightarrow j}\bigg].
\end{equation}
When the times goes to infinity, the system will approach the most likely state and we will have $P_i^{(n)} = p_i$, which requires
\begin{equation}
\sum_j\bigg[p_jT_{j \rightarrow i}A_{j \rightarrow i} - p_iT_{i \rightarrow j}A_{i \rightarrow j}\bigg]=0.
\end{equation}
Rearranging, we obtain a quite useful result
\begin{equation}
\frac{A_{j\rightarrow i}}{A_{i\rightarrow j}}=\frac{p_iT_{i\rightarrow j}}{p_jT_{j\rightarrow i}}
\end{equation}

\subsubsection{Brute force}
In the brute force Metropolis algorithm we want to check if the new position is more likely than the current position, and for that we calculate the probabilities $P(\vec{R})=|\Psi_T(\vec{R})|^2$ for both positions. We get rid off the transition probabilities setting $T_{i\rightarrow j}=T_{j\rightarrow i}$, and then end up with the plain ratio
\begin{equation}
w=\frac{P(\vec{R}_{new})}{P(\vec{R})}=\frac{|\Psi_T(\vec{R}_{new})|^2}{|\Psi_T(\vec{R})|^2}.
\end{equation}
$w$ will be larger than one if the new position is more likely than the current, and smaller than one if the current position is more likely than the new one. Metropolis handle this by accepting if the ratio $w$ is larger than a random number $r$ in the interval $[0,1]$, and rejecting if not:
\begin{equation}
\text{New position: }
\begin{cases} 
   \text{accept} & \text{if}\quad w > r \\
   \text{reject} & \text{if}\quad w \leq r.
\end{cases}
\end{equation}

\subsubsection{Importance sampling} \label{Importance_sampling}
The importance sampling technique is often refered to as Metropolis-Hastings algorithm. The approach is the same as for the brute force Metropolis algorithm, but we will end ut with a slightly more complicated acceptation criteria. To understand the details, we need to begin with the Fokker-Planck equation, which describes the time-evolution of the probability density function $P(R,t)$. In one dimension it reads
\begin{equation}
\frac{\partial P(R,t)}{\partial t} = D\frac{\partial}{\partial R}\bigg(\frac{\partial}{\partial R} - F\bigg)P(R,t).
\end{equation}
where $F$ is the drift force given by equation \ref{eq:drift_force} and $D$ is the diffusion coefficient, in this case equal $0.5$. Calculations of the analytical expression of the drift force $F$ for a shperical harmonic oscillator and $a=0$ can be found in appendix B.

\begin{equation}
	\label{eq:drift_force}
	F(R) = \frac{2 \nabla \psi_T}{\psi_T}
\end{equation}

Even though the probability density function can give a lot of useful information, an equation describing the motion of a such particle would be more appropriate for our purposes. Fortunately this equation exists, and satisfies the Fokker-Planck equation. The Langevin equation can be written as
\begin{equation}
\frac{\partial R(t)}{\partial t}=DF(R(t)) + \eta
\end{equation}
where $\eta$ can be considered as a random variable. This differential equation can be solved by applying the forward Euler method and introducing gaussian variables $\xi$
\begin{equation}
R_{new} = R + DF(R)\Delta t + \xi\sqrt{\Delta t}
\end{equation}
which will be used to update the position. This is an improved way of choosing the direction in which the particle is moved compared to the brute force algorithm, as the drift force $F(R)$ says something about which direction the particle is pushed in, and the choice of the new position is thus dependent on this.

Moreover we also need to update the acceptance criteria since we no longer ignore the transition probabilities. With the Fokker-Planck equation as base, the transition probabilities are given by Green's function
\begin{align}
T_{R\rightarrow R_{new}}&=G(R_{new},R,\Delta t)\notag\\
&=\frac{1}{(4\pi D\Delta t)^{3N/2}}\exp[-(R_{new} - R - D\Delta tF(R))^2/4D\Delta t] 
\end{align}
and the acceptance criteria becomes
\begin{equation}
r<\frac{G(R,R_{new},\Delta t)|\Psi_T(R_{new})|^2}{G(R_{new},R,\Delta t)|\Psi_T(R)|^2}.
\end{equation}



\subsection{Minimization methods}
When the interaction term is excluded, we know which $\alpha$ that corresponds to the energy minima, and it is in principle no need to try different $\alpha$'s. However, sometimes we have no idea where to search for the minimum point, and we need to try various $\alpha$ values to determine the lowest energy. If we do not know where to start searching, this can be a time consuming activity. Would it not be nice if the program could do this for us?

In fact there are multiple techniques for doing this, where the most complicated ones obviously also are the best. Anyway, in this project we will have good initial guesses, and are therefore not in need for the most fancy algorithms. 

\subsubsection{Gradient descent}
Perhaps the simplest and most intuitive method for finding the minima is the gradient descent method, which reads
\begin{equation}
\label{eq:GD}
\alpha^+=\alpha - \eta\cdot\frac{d\langle E_L(\alpha)\rangle}{d\alpha}.
\end{equation}
where $\alpha^+$ is the updated $\alpha$ and $\eta$ is the step size. The idea is that one finds the gradient of the energy with respect to a certain $\alpha$, and moves in the direction which minimizes the energy. This is repeated until one has found an energy minimum, where the energy minimum is defined as either where $\frac{d\langle E_L(\alpha)\rangle}{d\alpha}$ is smaller than a given tolerance, or the $\alpha$ energy fluctuates around a value, and thus changes minimally.
\par 
\vspace{3mm}

To implement equation \ref{eq:GD}, we need an expression for the derivative of $E_L$ with respect to alpha

\begin{equation}
	\label{eq:E_L_der_wrt_alpha}
	\bar{E_{\alpha}} = \frac{d \langle E_L (\alpha) \rangle}{d \alpha}
\end{equation}

By using the expression for the exprectation value for the local energy $ \langle E_L (\alpha) \rangle$ in equation \ref{eq:exp_EL} 

\begin{equation}
	\label{eq:exp_EL}
	\langle E_L (\alpha) \rangle = \frac{ \langle \psi_T(\alpha) | H | \psi_T(\alpha)  \rangle}{ \langle \psi_T(\alpha)  |  \psi_T(\alpha)  \rangle }
\end{equation}

and applying the chain rule of differentiation, it can be shown that equation \ref{eq:E_L_der_wrt_alpha} reduces to equation \ref{eq:E_L_der_wrt_alpha_expression}

\begin{equation}
	\label{eq:E_L_der_wrt_alpha_expression}
	\bar{E_{\alpha}} = 2 [\langle E_L (\alpha)  \frac{ \bar{\psi_{\alpha}}}{\psi_{\alpha}}\rangle - \langle E_L (\alpha) \rangle \langle \frac{\bar{\psi_{\alpha}}}{\psi_{\alpha}} \rangle ]
\end{equation}

where

\begin{equation}
	\bar{\psi_{\alpha}} = \frac{d \psi (\alpha)}{d \alpha}
\end{equation}

The algorithm of this minimization method is thus as follows:

\lstset{basicstyle=\scriptsize}
\begin{lstlisting}

for(number max number of iterations with minimizing)
	
	do M Monte Carlo cycles
	calculate E_L and d E_L/d alpha 
	
	Check if d E_L/d alpha < eps or alpha fluctuation over the last 5 steps is < eps
	
	
		if yes, print optimal alpha and break loop
		if no, continue to next iteration


\end{lstlisting}

\subsection{Blocking method}
\label{Blocking}

As described in section \ref{Error_estimation}, we need a method to give a proper estimation of the variance $\sigma^2$ of the points in our experiment, preferably without calculating the double loop from the expression of the covariance in equation \ref{eq:covariance}. 
\par 
\vspace{3mm}

One method that can be used, is the blocking method, which is quite fast and can handle large data sets. Say that we have a data set ${x_1, ...,x_i,..., x_n}$ from a experiment, which in our case will be the estimations of the local energies for each Monte Carlo sycle. The mean of this data set is $m$, and we want to estimate the variance of this data set, $\sigma^2 (m)$.  By defining a function $\gamma_{i,j}$ as 

\begin{equation}
	\gamma_{i,j} = \langle x_i x_j \rangle - \langle x_i \rangle \langle x_j \rangle
\end{equation}

we see that $\sigma^2 (m)$ can be written in terms of $\gamma_{i,j}$ 

\begin{equation}
	\sigma^2(m) = \frac{1}{n^2}\sum_{i,j = 1}^{n} \gamma_{i,j}
\end{equation}


If we transform the data set ${x_1, ...,x_i,..., x_n}$ by taking the mean of two neigbouring points in the following way
\begin{equation}
	x'_i = \frac{1}{2}(x_{2i} + x_{2i+1})
\end{equation}

 the number of points in the transformed data set $n' = \frac{1}{2} n$, while the mean $m'$ is preseverd, such that $m' = m$, and therefore the variance is also the same for the transformed data set, $\sigma^2(m') = \sigma^2(m)$. Using the three point formula and after $l$ transformations, the $\gamma_h$, $h= |i-j|$ can be estimated with $\hat{\gamma}$ (??) written as 
 
 \begin{equation}
 	\gamma^{l+1}(h) = \frac{1}{4} \gamma^{l}(2h-1) + \frac{1}{2}\gamma^{l}(2h) + \frac{1}{4} \gamma^{l}(2h+1) 
 \end{equation}

Using the estimated  $\hat{\gamma}$ the estimated variance $ \hat{\sigma^2}$ can thus be calculated for the transformed data set.

 The transformation of data set  is continues, for each new data set $\hat{\gamma}^l(h=1)$ and $\hat{\sigma}^{(2),l}$ until the number of points $n'$ in the transformed data is $\leq 2$. 
 
 Using the set of $\hat{\gamma}^l(h=1)$ and $\hat{\sigma}^{(2),l}$, the chi-squared distribution is calcualted, and the value of $l$ where the chi-square drops below a tolerance. For this value of $l$, $\sigma^2 = \hat{\sigma^2}$.


\section{Results} \label{Results}

\subsection{$E_L$ calculation and CPU-time}\label{CPU}
For the brute force Metropolis algorithm we developed both an analytical and a numerical method to calculate the local energy. In table (\ref{tab:BFmet}) we present the results from these calculations and the performance. The results from the calculations with the Metropolis-Hastings algorithm are presentet in table \ref{tab:ISmet}. All the measurements are done in three dimensions with $1e6$ Monte Carlo cycles. $a$ is fixed to zero.

\begin{table} [H]
	\centering
	\caption{The local energy calculated for $a=0$  without hard-sphere interaction with brute force Metropolis algorithm, for both the analytical and numerical local energy calculation. The results are compared to the exact answer, obtained from equation \ref{eq:Energy_exact}. The number of Monte Carlo cycles is fixed to $M=1e6$, and the performance is presented along with the local energies. }
	\begin{tabularx}{\textwidth}{X|XX|XX|X} \hline
		\label{tab:BFmet}
		& \multicolumn{2}{X}{\textbf{Analytical}} & \multicolumn{2}{X}{\textbf{Numerical}} & Exact \\
		$N$ & $\langle E_L\rangle$ [$\hbar\omega_{HO}$] & CPU-time [s] & $\langle E_L\rangle$ [$\hbar\omega_{HO}$] & CPU-time [s]& $\langle E_L\rangle$ [$\hbar\omega_{HO}$]\\ \hline
		1 & 1.5000 & 0.15420 & 1.49999 & 0.65714 & 1.5000\\
		10 & 15.000 & 0.54785 & 14.9999 & 9.9844 & 15.000\\
		100 & 150.00 & 13.573 & 149.999 & 2743.4 & 150.00\\
		500 & 750.00 & 282.79 & 749.996 & 2.8744e5 & 750.00\\ \hline
	\end{tabularx}
\end{table}

\begin{table} [H]
	\centering
	\caption{The local energy calculated for $a=0$  without hard-sphere interaction with the Metropolis-Hastings algorithm with analytical local energy calculation. The results are compared to the exact answer, obtained from equation \ref{eq:Energy_exact}. The number of Monte Carlo cycles is fixed to $M=1e6$, and the performance is presented along with the local energies.}
	\begin{tabularx}{\textwidth}{X|XX|X} \hline
		\label{tab:ISmet}
		& \textbf{Analytical}  & & \textbf{Exact}\\
		$N$ & $\langle E_L\rangle$ [$\hbar\omega_{HO}$] & CPU-time [s] & $\langle E_L\rangle$ [$\hbar\omega_{HO}$]\\ \hline
		1 & 1.5000 & 0.26693  & 1.5000 \\
		10 & 15.000 &  0.54930 & 15.000 \\
		100 & 150.00 & 16.117 & 150.00 \\
		500 & 750.00 & 292.10 & 750.00 \\ \hline
	\end{tabularx}
\end{table}


 \subsection{$E_L$ as function of the variational parameter $\alpha$}

\begin{figure} [H]
	\centering
	\includegraphics[scale=0.65]{images/energy.png}
	\caption{The local energy $E_L$ calculated with the brute force Metropolis algorithm, as a function of the variational parameter $\alpha$}
	\label{fig:EL_as_func_of_alpha}
\end{figure} 

\begin{figure} [H]
	\centering
	\includegraphics[scale=0.65]{images/variance.png}
	\caption{The variance of the local energy $E_L$ calculated with the brute force Metropolis algorithm, as a function of the variational parameter $\alpha$. \textcolor{red}{CALCULATED with imprecise var}}
	\label{fig:variance_EL_as_func_of_alpha}
\end{figure} 

\subsection{Acceptance ratios}
We study the acceptance ratio for the brute force and importance sampling algorithms, with no interaction. These calculations are done with ten particles in three dimentions. The number of Monte Carlo cycles is fixed to $M=1e6$, and the variational parameter $\alpha$ is equal to $0.5$.

\subsubsection{Importance sampling dependence on timestep }
The acceptance ratio for the importance sampling algorithm as a function of the timestep $\delta t$ is shown in figure \ref{fig:acceptance_IS_timestep}. 

\begin{figure} [H]
	\centering
	\includegraphics[scale=0.65]{images/acceptance_IS.png}
	\caption{Acceptance ratio for different choices of the timestep $\delta t$  with the Metropolis-Hastings algorithm.}
	\label{fig:acceptance_IS_timestep}
\end{figure} 

\subsubsection{Brute force dependence on stepsize}
The acceptance ratio for the brute force algorithm as a function of the stepsize is shown in figure \ref{fig:acceptance_BF_stepsize}. 

\begin{figure} [H]
	\centering
	\includegraphics[scale=0.65]{images/acceptance_BF.png}
	\caption{Acceptance ratio for different choices of the stepsize used in the brute force algorithm}
	\label{fig:acceptance_BF_stepsize}
\end{figure} 

 \subsection{Variance calculation}

In table \ref{tab:variance_analysis}, we present the results of the local energy calculation with proper error evaluation of statistical error. The statistical error as found with the Blocking method is also compared to the simple way of approximating the variance by $\sigma^2 \approx \langle E_L^2 \rangle - \langle E_L \rangle^2$. 

\begin{table} [H]
	\caption{The local energy calculated for $a=0$  without hard-sphere interaction with the Metropolis-Hastings algorithm with analytical local energy calculation, this time with proper evaluation of the statistical error.  The calculations are run in three dimentions with $1e6$ Monte Carlo cycles, and the variational paramameter $\alpha=0.6$}
	\begin{tabularx}{\textwidth}{X|XXXX} \hline
		\label{tab:variance_analysis}
		$N$ & $\langle E_L\rangle$ [$\hbar\omega_{HO}$] & $\sigma^2$ Blocking & $\sigma^2 \approx \langle E_L^2 \rangle - \langle E_L \rangle^2$ \\ \hline
				1 & 1.5000 & 5.0102e-2  & 5.0100e-2 \\
				10 & 15.000 & 4.9701e-1 & 4.9678e-1  \\
				100 & 150.00 & 5.2417 & 5.2184\\
				500 & 750.00 & 9.0710 & 8.4831 \\ \hline
	\end{tabularx}
\end{table}

\subsection{VMC with repulsive interaction}
In table \ref{tab:EL_calc_repulsive_pot}, the results from the calculation of the elliptical trap are presented. The variational parameter $\alpha$ was varied manually to find the minimum of the local energy $E_L$.

\begin{table} [H]
	\centering
	\caption{The local energy $E_L$ calculated for different $\alpha$, with $a=0.0043$  with hard-sphere interaction,  elliptical trap and $\beta=2.82843$, with the brute force algorithm with analytical local energy calculation.  The calculations are run in three dimentions with $1e6$ Monte Carlo cycles}
	\begin{tabularx}{\textwidth}{X|XXXXX} \hline
		\label{tab:EL_calc_repulsive_pot}
		$N$ & $\alpha$ = 0.2 & $\alpha$ = 0.3  & $\alpha$ = 0. 35 & $\alpha$ = 0. 4 & $\alpha$ = 0. 5   \\ \hline
		1 & 1.96397 & 1.70441  & 1.68752 & 1.70148 & 1.79225\\
		10 & 19.6679 & 17.1322 & 16.9584 &17.1359 & 18.0973\\
		100 & 255.34 & 253.248 & 262.777 & 275.639 & 307.241\\ \hline
	\end{tabularx}
\end{table}

\subsubsection{Gradient Decent}
The gradient decent method was then used to find the minimum in the local energy in the interacting case. The results are presented in table \ref{tab:EL_calc_gradientdecent}.

\begin{table} [H]
	\centering
	\caption{The local energy $E_L$ calculated with the gradient decenet method, with $a=0.0043$  with hard-sphere interaction,  elliptical trap and $\beta=2.82843$, with the brute force algorithm with analytical local energy calculation. The table shows the optimal $\alpha$ found by the method, and the resulting local energy. The calculations are run in for  three dimentions with $1e6$ Monte Carlo cycles.}
	\begin{tabularx}{\textwidth}{X|XXX} \hline
		\label{tab:EL_calc_gradientdecent}
		$N$ & $\alpha $ & $E_L$  & $\frac{\partial E_L}{\partial \alpha}$   \\ \hline
		1 & 0.349923 & 1.68819 & 0.000853655 \\
		10 &0.345019 & 16.9141 & -0.365277 \\ \hline
	\end{tabularx}
\end{table}



\subsection{Onebody density}
In section \ref{sec:OB_theory} we presented a couple of ways computing the onebody density. Because both should give the same result, we selected the simplest one, which is the method with the bins. We conduct the investigations in elliptic traps, with 10 particles, 3 dimensions and 1e6 Monte Carlo cycles. With the optimal parameter $\alpha$ for the ground state wave function and for three different choices of $a$, the onebody density plots are found in figure (\ref{fig:ob0}-\ref{fig:ob1}).

\begin{figure} [H]
    \centering
    \includegraphics[scale=0.65]{images/ob.png}
    \caption{The one body density of the bosonic system. $a=0$ is run with the optimal variational parameter $\alpha = 0.5$, while the $a=0.0043$ is run with $\alpha = 0.3450$. As lenght is dimentionless, both the density $\rho$ and the length $r$ are dimentionless}
    \label{fig:ob0}
\end{figure}

\section{Discussion} \label{Discussion}

When comparing the results from the brute force Monte Carlo calculation with spherical harmonic oscillator and no interaction, as shown in table \ref{tab:BFmet}, we observe that in the case when we use an analytical expression for $E_L$, the simulation results yields the exact energy, as obtained from equation \ref{eq:Energy_exact}. When using the numerically calculated $E_L$, as described in section \ref{Numerical_calc_E_L}, the calculated results deviate slightly from the exact answer. Also when comparing the CPU time difference between these two calculations, the numerically calculated $E_L$ spends vastly more time on the calculations compared to the analytically calculated $E_L$-case. The time difference is as large as a factor of $10^3$ for the 500 particles, thee dimentions case. This makes it preferable in both the aspect of time and accuracy to use the analytically calculated $E_L$ in our calculations. However, not all trial wave functions $\psi_T$ or Hamiltonians H will yield an $E_L$ which is analytically possible to calculate, and in this case, one should have a functioning algorithm for numerical calculation of $E_L$ as well.
\par 
\vspace{3mm}

When comparing the results from the brute force and Hastings Metropolis algorithms, shown in tables \ref{tab:BFmet} and \ref{tab:ISmet}, the reader can oberve that the Hastings algorithm reprocuces the exact energies, as the brute force algorithm does, but that Hastings is slightly slower. This sligt time decrease is due to the fact that while the brute force algorithm picks a proposed new step at random, the Hasting algorithm makes a more educaded move by calculating the drift force $F$ on the particle, and decide what direction the particle is most likely to move in, as described in section \ref{Importance_sampling}. While this calculation spends slightly more CPU time, it is rewarded by the Hastings algorithm having a higher acceptance rate of the proposed moves compared to the brute force metropolis. This can be seen from figures \ref{fig:acceptance_BF_stepsize} and \ref{fig:acceptance_IS_timestep}. The acceptance ratio of the Hastings algorithm ifor a timestep $\Delta t =1$ is much higher than the acceptance ratio for brute force with a stepsize $r=1$. \textcolor{red}{This results in the Hastings algorithm moving faster towards the minima compared to the brute force method. This effect, however, is not that visible in our project, but makes a vital difference for larger Monte Carlo simulations}.
\par 
\vspace{3mm}

Sill studying figure \ref{fig:acceptance_IS_timestep} and \ref{fig:acceptance_BF_stepsize}, we observe that the importance sampling algorithm has a high accseptance rate for intermediate timesteps $\Delta t$, but the acceptance rate drops seemingly exponentially for higher timesteps. Comparing this to the brute force acceptance rate, the drop in the acceptance rate is more like a lineary decreasing curve. \textcolor{red}{This is because the acceptance probability for brute force to do a step in the "wrong direction" is still present, while moving in the wrong direction with importance sampling is much less probable, and therefore the acceptance rate decreases rapidly with increased timestep }.
\par 
\vspace{3mm}

In table \ref{tab:variance_analysis}, we repeated the calculation of the no interaction spherical potential with importance sampling, but this time we included a proper evaluation of the error. We compared the variances obtained from the simple, but incorrect way of calculating error, as shown in equation \ref{eq:variance_simplified} to a proper variance calculation using the Blocking method, by evaluation equation \ref{eq:sample_var_all_exp}. As expected, the simplified variance calculation underestimates the statistical error, as it does not include the covariance term in equation \ref{eq:sample_var_all_exp}. However, the deviation between the two is small for few number of particles. We would say that as blocking is a quick way of calculating the statistical error, it should be done when presenting results, but the simplified error calculation could still give a quick estimation of the magnitude of the variance in a simulation.
\par 
\vspace{3mm}

Must benchmark results from e and f!


\section{Conclusion}

In this project, we have fou


\newpage
\section{References}
\begingroup
\renewcommand{\section}[2]{}
\begin{thebibliography}{}
	\bibitem{MHJ15}
	Morten Hjorth-Jensen.
	Computational Physics 2: Variational Monte Carlo methods, Lecture Notes Spring 2018.
	Department of Physics, University of Oslo.
	February 2015.
	\bibitem{DuBois}
	J. L. DuBois and H. R. Glyde, H. R., \emph{Bose-Einstein condensation in trapped bosons: A variational Monte Carlo analysis}, Phys. Rev. A \textbf{63}, 023602 (2001).
	\bibitem{Nilsen}
	J. K. Nilsen,  J. Mur-Petit, M. Guilleumas, M. Hjorth-Jensen, and A. Polls, \emph{Vortices in atomic Bose-Einstein condensates in the large-gas-parameter region}, Phys. Rev. A \textbf{71}, 053610 (2005).
	\bibitem{JE2016}
	Jesse Emspak, \emph{States of Matter: Bose-Einstein Condensate}, LiveScience (2016)
	\url{https://www.livescience.com/54667-bose-einstein-condensate.html}
	Downloaded March 15th 2018
	\bibitem{SP}
	Sidney Perkowitz \emph{Bose-Einstein condensate} Encyclopaedia Britannica 
	\url{https://www.britannica.com/science/Bose-Einstein-condensate}
	Downloaded March 15th 2018
\end{thebibliography}
\endgroup

\newpage

\section*{Appendix A} \label{appendix_A}

For the 
\subsection{Without repulsive interaction}
We calculated the analytical expression for the local energy $E_L$, as given by \ref{eq:Local_energy}, for the non-interaction ($a=0$) case for the spherical harmonic oscillator. In this case, the wave trial function for only consists of the one body part, and is thus for $N$ particles given by:

\begin{equation}
	\label{eq:WF_nointeract}
	\Psi_T(\vec{r}) = \prod_i^N e^{-\alpha(x_i^2 + y_i^2 + \beta z_i^2)}
\end{equation}

We now want to calculate the analytical expressions for $E_L$ for one particle and one dimention, and $N$ particles and three dimentions.

\subsubsection{One particle, one dimension}

From \ref{eq:WF_nointeract}, the trial wave function for one particle and one dimention is as follows:

\begin{equation}
	\label{eq:WF_1dim_1N}
	\Psi_T(x) = e^{-\alpha x^2} 
\end{equation}

We are starting from the local energy equation (equation (\ref{eq:Local_energy})), where we need to take the second derivative
\begin{align}
\frac{d\Psi_T}{dx}&=-2\alpha xe^{-\alpha x^2}\\
\frac{d^2\Psi_T}{dx^2}&=-2\alpha e^{-\alpha x^2}+4\alpha^2x^2e^{-\alpha x^2}.
\end{align}
To get a neat expression, we use the dimensionless Hamiltonian with spherical harmonic oscillator potential, and obtain
\begin{align}
E_L(\alpha)&=-\frac{1}{2}(-2\alpha + 4\alpha^2x^2)+\frac{1}{2}x^2\notag\\
&=\alpha+\Big(\frac{1}{2}-2\alpha^2\Big)x^2
\end{align}

\iffalse
In the Hamiltonian, the Laplace-operator reduces to the partial derivative in the x-direction, $\frac{\partial^2 }{\partial x^2}$, and $E_L$ is 

\begin{equation}
\begin{aligned}
E_L(x) &=  \frac{1}{\Psi_T(x)}\hat{H}\Psi_T(x) \\ 
			 & =  e^{\alpha x^2} (-\frac{\hbar^2}{2m}\frac{\partial^2 }{\partial x^2} + \frac{1}{2} m \omega_{HO}^2 x^2) e^{-\alpha x^2}  \\
			 & = e^{\alpha x^2} [-\frac{\hbar^2}{2m}\frac{\partial^2 }{\partial x^2} (e^{-\alpha x^2}) + \frac{1}{2} m \omega_{HO}^2 x^2 (^{-\alpha x^2})]
\end{aligned}
\end{equation}

Double differentiation of $ e^{-\alpha x^2}$ yields

\begin{equation}
\frac{\partial^2}{\partial x^2} (e^{-\alpha x^2}) = e^{-\alpha x^2} 2 \alpha (2x^2 -1)
\end{equation}

Inserting this into the expression for $E_L$ gives

\begin{equation}
E_L(\alpha) = -\frac{\hbar^2}{m} \alpha (2x\alpha^2 -1) + \frac{1}{2} m\omega_{HO}^2x^2
\end{equation}
\fi

\subsubsection{$N$ particles, three dimensions}

When extending the non-interaction case to $N$ particles and three dimentions, the trial wave function will take the form listed in \ref{eq:WF_nointeract}, and the Hamiltonian will be as listed in \ref{eq:Hamilton}, with the dimensionless spherical harmonic oscillator potential $\frac{1}{2}r_i^2$, as listed in \ref{eq:V_ext}.

\iffalse
The local energy is then

\begin{equation}
	E_L(\alpha) = \prod_i^N e^{\alpha(x_i^2 + y_i^2 + \beta z_i^2)} \sum_i [ -\frac{\hbar^2}{2m} \nabla_i^2 + \frac{1}{2}m\omega_{HO}^2(x_i^2 + y_i^2 + z_i^2)] \prod_i^N e^{-\alpha(x_i^2 + y_i^2 + \beta z_i^2)}
\end{equation}

Double differentiation of $\Psi_T$ with respect to x gives

\begin{equation}
	\frac{\partial^2}{\partial x^2} (\prod_i e^{-\alpha(x_i^2 + y_i^2 + \beta z_i^2)}) = \prod_i e^{-\alpha(x_i^2 + y_i^2 + \beta z_i^2)} [\sum_{i,j} 4 \alpha^2 x_i x_j -2 \alpha N ]
\end{equation}

Double differentiation with respect to y will give similar answer, while for z there will be a factor $\beta$ difference

\begin{equation}
\frac{\partial^2}{\partial z^2} (\prod_i e^{-\alpha(x_i^2 + y_i^2 + \beta z_i^2)}) = \prod_i e^{-\alpha(x_i^2 + y_i^2 + \beta z_i^2)} [\sum_{i,j} 4 \alpha^2 \beta^2 y_i y_j -2 \alpha \beta N ]
\end{equation}

By including the answers from the differentiations in the x,y and z directions and simplifying the expression results in the following expression for the local energy

\begin{equation}
\begin{aligned}
	E_L(\alpha) & = -\frac{\hbar^2}{2m} N [ \sum_{i,j} 4 \alpha^2 x_i x_j + \sum_{i,j} 4 \alpha^2 y_i y_j + \sum_{i,j} 4 \alpha^2 \beta^2 z_i z_j] +  \\ & \frac{2 \hbar^2}{m}\alpha N^2 + \sum_i \frac{1}{2}m\omega_{HO^2}(x_i^2 + y_i^2 + z_i^2) + \frac{\hbar^2}{m}\beta \alpha N^2
\end{aligned}
\end{equation}
\fi


For this case it might be better to use spherical coordinates, where the Laplace operator is given by
\begin{equation}
\nabla^2=\frac{1}{r^2}\frac{\partial}{\partial r}\bigg(r^2\frac{\partial}{\partial r}\bigg)
\end{equation}
when we only take the radial part into account. We transform the wavefunction from a product to a function with a sum in the exponent, and calculate the first derivative
\begin{equation}
\frac{\partial\Psi_T}{\partial r_j}=-2\alpha r_j \exp\bigg[-\alpha\bigg(\sum_i r_i^2\bigg)\bigg].
\end{equation}
After adding a $r_j^2$, we differentiate the expression again
\begin{equation*}
\frac{\partial}{\partial r_j}\bigg(-2\alpha r_j^3\exp\Big[-\alpha\Big(\sum_i r_i^2\Big)\Big]\bigg)
=(-6\alpha r_j^2 + 4\alpha^2r_j^4)\exp\Big[-\alpha\Big(\sum_i r_i^2\Big)\Big]
\end{equation*}
and we obtain
\begin{equation}
\frac{\nabla_j^2\Psi_T}{\Psi_T}=-6\alpha+4\alpha^2r_j^2.
\end{equation}
Again we get the local energy from the Hamiltonian with a spherical harmonic oscillator potential. 
\begin{align}
E_L(\alpha)&=\sum_j-\frac{1}{2}(-6\alpha+4\alpha^2r_j^2) + \frac{1}{2}r_j^2\notag\\
&=3N\alpha + \Big(\frac{1}{2}-2\alpha^2\Big)\sum_{j=1}^Nr_j^2
\end{align}

\subsubsection{General}
If one now studies the local energy expressions for one particle in one dimension and $N$ particles in three dimensions, one can see a pattern and it is easy to imagine that there exists a general expression for the local energy. At least for 1, 2, and 3 dimensions (probably all), it can be shown that
\begin{equation}
E_L(\alpha)=dim\cdot N\cdot \alpha+\Big(\frac{1}{2}-2\alpha^2\Big)\sum_jr_j^2
\end{equation}

\subsection{With repulsive interaction}
As mention in the theory section, the calculations get more complicated when adding the interaction, and we are in fact not able to find the local energy analytical. However, we can simplify the local energy expression and hopefully gain some speed up compared to when we do all the calculations numerically. 

We start with defining the onebody part of the wavefunction as $\Phi$ and the Jastrow factor as $\exp\big(u(r_{ij})\big)$ such that the total trial wavefunction becomes
\begin{equation}
\Psi_T(\vec{r}_1,\hdots\vec{r}_N)=\Big[\prod_i\Phi(\vec{r}_i)\Big]\exp\Big(\sum_{i<j}u(r_{ij})\Big)
\end{equation}

Thereafter we calculate the term connected to the kinetic part of the Hamiltonian,
\begin{equation}
\frac{\nabla_k^2\Psi_T(\vec{r})}{\Psi_T(\vec{r})},
\label{eq:ham_kin}
\end{equation}
which is the most difficult part. We apply the product rule and obtain the following expression for the first derivative
\begin{align}
\nabla_k\Psi_T(\vec{r})&=\nabla_k\Phi(\vec{r}_k)\Big[\prod_{i\neq k}\Phi(\vec{r}_i)\Big]\exp\Big(\sum_{i<j}u(r_{ij})\Big)\notag\\
&\phantom{=}+\prod_i\Phi(\vec{r}_i)\exp\Big(\sum_{i<j}r(r_{ij})\Big)
\sum_{j\neq k}\nabla_k u(r_{ij})
\end{align}

For the second derivative we get five terms in total where two of them are cross terms and therefore equal. 
\begin{align}
\nabla_k^2\Psi_T(\vec{r})&=\nabla_k^2\Phi(\vec{r}_k)\Big[\prod_{i \neq k}\Phi(\vec{r}_i)\Big]\exp\Big(\sum_{i<j}u(r_{ij})\Big)\notag\\
&\phantom{=}+\nabla_k\Phi(\vec{r}_k)\Big[\prod_{i \neq k}\Phi(\vec{r}_i)\Big]\exp\Big(\sum_{i<j}u(r_{ij})\Big)\sum_{j\neq k}\nabla_ku(r_{ij})\notag\\
&\phantom{=}+\nabla_k\Phi(\vec{r}_k)\Big[\prod_{i \neq k}\Phi(\vec{r}_i)\Big]\exp\Big(\sum_{i<j}u(r_{ij})\Big)\sum_{j\neq k}\nabla_ku(r_{ij})\\
&\phantom{=}+\prod_i\Phi(\vec{r}_i)\exp\Big(\sum_{i<j}u(r_{ij})\Big)\sum_{i\neq k}\nabla_ku(r_{ij})\sum_{j\neq k}\nabla_ku(r_{ij})\notag\\
&\phantom{=}+\prod_i\Phi(\vec{r}_i)\exp\Big(\sum_{i<j}u(r_{ij})\Big)\sum_{j\neq k}\nabla_k^2u(r_{ij})\notag
\end{align}

To simplify this, we do a change of variables
\begin{equation}
\frac{\partial}{\partial \vec{r}_k}=\frac{\partial}{\partial \vec{r}_k}\frac{\partial r_{kj}}{\partial r_{kj}}=\frac{\partial r_{kj}}{\partial \vec{r}_k}\frac{\partial}{\partial r_{kj}}=\frac{(\vec{r}_k-\vec{r}_j)}{r_{kj}}\frac{\partial}{\partial r_{kj}}
\end{equation}
where we have used that
\begin{equation}
\frac{\partial r_{kj}}{\partial \vec{r}_k}=\frac{\vec{r}_k - \vec{r}_j}{|\vec{r}_k - \vec{r}_j|}=\frac{\vec{r}_k - \vec{r}_j}{r_{kj}}.
\end{equation}
This leads to
\begin{equation}
\nabla_ku(r_{ij})=\frac{\partial r_{kj}}{\partial \vec{r}_k}\frac{\partial}{\partial r_{kj}}\big(u(r_{ij})\big)=\frac{\vec{r}_k - \vec{r}_j}{|\vec{r}_k - \vec{r}_j|}u'(r_{kj})
\end{equation}
and
\begin{align*}
\nabla_k^2u(r_{ij})&=\nabla_k\bigg(\frac{\vec{r}_k - \vec{r}_j}{|\vec{r}_k - \vec{r}_j|}u'(r_{kj})\bigg)\\
&=\frac{1}{|\vec{r}_k-\vec{r}_j|}\frac{\partial}{\partial r_{kj}}\big(u(r_{kj})\big)+\frac{\partial^2}{\partial r_{kj}^2}\big(u(r_{kj})\big)\\
&\phantom{=}+\frac{(\vec{r}_k-\vec{r}_j)(\vec{r}_k-\vec{r}_j)}{|\vec{r}_k-\vec{r}_j|^3}\frac{\partial}{\partial r_{kj}}\big(u(r_{kj})\big)
\end{align*}
where we can simplify the last term
\begin{equation}
\frac{(\vec{r}_k-\vec{r}_j)(\vec{r}_k-\vec{r}_j)}{|\vec{r}_k-\vec{r}_j|^3}=\frac{(\vec{r}_k - \vec{r}_j)(\vec{r}_k - \vec{r}_j)}{(\vec{r}_k - \vec{r}_j)(\vec{r}_k - \vec{r}_j)|\vec{r}_k - \vec{r}_j|}=\frac{1}{|\vec{r}_k - \vec{r}_j|}.
\end{equation}
Finally we can write out the expression from equation (\ref{eq:ham_kin}) 
\begin{align}
\frac{\nabla_k^2\Psi_T}{\Psi_T}&=\frac{\nabla_k^2\Phi(\vec{r}_k)}{\Phi(\vec{r}_k)} + 2\frac{\nabla_k\Phi(\vec{r}_k)}{\Phi(\vec{r}_k)}\bigg(\sum_{j\neq k}\frac{(\vec{r}_k-\vec{r}_j)}{r_{kj}}u'(r_{kj})\bigg)\notag\\
&\phantom{=}+\sum_{ij\neq k}\frac{(\vec{r}_k-\vec{r}_j)(\vec{r}_k-\vec{r}_j)}{r_{kj}r_{ki}}u'(r_{kj})u'(r_{ki})\\
&\phantom{=}+\sum_{j\neq k}\bigg(\frac{(\vec{r}_k-\vec{r}_j)}{r_{kj}}u''(r_{kj})+\frac{2}{r_{kj}}u'(r_{kj})\bigg)\notag
\end{align}
Now we can easily find a local energy expression using the general expression for the local energy:
\begin{align}
E_L&=\frac{1}{\Psi_T}\bigg(\sum_i\Big(-\frac{1}{2}\nabla_i^2+V_{ext}(\vec{r}_i)\Big) + \sum_{i<j}V_{int}(\vec{r}_i,\vec{r}_j)\bigg)\Psi_T\notag\\
&=\sum_i\Big(-\frac{\nabla_i^2\Psi_T}{2\Psi_T}+V_{ext}(\vec{r}_i)\bigg)+\sum_{i<j}V_{int}(\vec{r}_i,\vec{r}_j)
\end{align}

\section*{Appendix B} \label{sec:appendix_b}

This section shows the analytical calculations for the drift force $F$, as given by equation \ref{eq:drift_force} for the non-interaction case.

\subsubsection{$N$ particles, 1 dimention}

The no interaction, N particle trial wave function $\psi_T$ is given by

\begin{equation}
	\prod_i e^{-\alpha x_i^2}
\end{equation}

 The drift force F is then

\begin{align}
	F = \frac{2 \nabla ( \prod_i e^{-\alpha x_i^2} )}{ \prod_i e^{-\alpha x^2} }
\end{align}

The differentiation $\nabla \rightarrow \frac{d }{d x}$, as we operate in one dimention. The differentiation of $\psi_T$ yields

\begin{equation}
	\label{eq:differentiation_WF}
	\frac{d}{d x} ( \prod_i e^{-\alpha x_i^2}) =  \prod_i e^{-\alpha x_i^2} \sum_i (-2\alpha x_i)
\end{equation}

This yields the drift force $F$

\begin{equation}
	F =  2* \prod_i e^{\alpha x_i^2}   \prod_i e^{-\alpha x_i^2} \sum_i (-2\alpha x_i) = - 4 \alpha \sum_i x_i
\end{equation}

\subsubsection{$N$ particles, 3 dimentions}

In the $N$ particles, $3$ dimentions case, the trial wave function $\psi_T$ takes the following form

\begin{equation}
	\psi_T = \prod_i e^{- \alpha (x_i^2 + y_i^2 + \beta z_i^2)} 
\end{equation}

As we operate in three dimentions, $\nabla = (\frac{d}{dx}, \frac{d}{dy}, \frac{d}{dy})$. The differentiation in one dimention of $\psi$ is shown in equation \ref{eq:differentiation_WF}, and the results are similar for the differentiation with respect to $y$ and $z$, the only difference being an extra factor $\beta$ in the expression for $\frac{d \psi_T}{d z}$, due to the $\beta$ in the expression for $\psi_T$. Thus the drift force F for $N$ particles in three dimentions is 

\begin{equation}
	F = -4 \alpha (\sum_i xi, \sum_i y_i, \sum_i \beta z_i)
\end{equation}

\section*{Appendix C} \label{sec:appendix_c}
In the theory part we claimed that the Hamiltonian could be written as
\begin{equation}
H=\sum_i\bigg(\frac{1}{2}\Big(-\nabla^2 + x_i^2 + y_i^2 + \gamma^2z_i^2\Big)\bigg)+\sum_{i<j}V_{int}(\vec{r}_i,\vec{r}_j)
\end{equation}
with
\begin{equation*}
\gamma=\beta=\frac{\omega_z}{\omega_{HO}}
\end{equation*}
for an elliptical harmonic oscillator potential with repulsive interaction. Let us start from scratch, where the unscalled Hamiltonian for an elliptical harmonic oscillator reads
\begin{equation*}
H=\sum_i\bigg(-\frac{\hbar^2}{2m}\nabla_i^2+\frac{1}{2}m\Big(\omega_{HO}^2(x_i^2+y_i^2)+\omega_z^2z_i^2\Big)\bigg)+\sum_{i<j}V_{int}(\vec{r}_i, \vec{r}_j).
\end{equation*}
We then scale the entire equation with respect to $\hbar\omega_{HO}$
\begin{equation*}
\frac{H}{\hbar\omega_{HO}}=\sum_i\bigg(-\frac{\hbar}{2m\omega_{HO}}\nabla_i^2+\frac{1}{2}\frac{m}{\hbar}\omega_{HO}(x_i^2+y_i^2)+\frac{1}{2}\frac{m}{\hbar}\frac{\omega_z^2}{\omega_{HO}}z_i^2\bigg)+\sum_{i<j}V_{int}(\vec{r}_i, \vec{r}_j)
\end{equation*}
where we can take $H'=H/\hbar\omega_{HO}$ as the dimensionless energy. Further we scale all the lengths in the same way
\begin{equation*}
x_i^2=(x_i')^2\cdot a_{HO}^2=(x_i')^2\cdot\frac{\hbar}{m\omega_{HO}},
\end{equation*}
and we finally obtain
\begin{equation}
H'=\sum_i\frac{1}{2}\bigg(-\nabla_i^2+(x_i')^2+(y_i')^2+\frac{\omega_z^2}{\omega_{HO}^2}(z_i')^2\bigg)+\sum_{i<j}V_{int}(\vec{r}_i, \vec{r}_j)
\end{equation}
which is the Hamiltonian that we were hunting. We know how the z-component is affected by $\beta$, so $\beta$ has to be equal to $\omega_z/\omega_{HO}$.

\end{document}
